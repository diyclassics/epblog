{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aristophanic wordclouds  \n",
    "*An 'Exploratory Philology' code notebook showing how to construct wordclouds of key words from Ancient Greek text. See full blog post here: [TK](#).*  \n",
    "  \n",
    "[P.J. Burns](https://diyclassics.github.io/)  \n",
    "10.14.2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Aristophanes's *Clouds*, we find a certain Strepsiades who joins a philosophy-rhetoric school—The Phrontisterion—in order to learn the arts of Strong and Weak Argumentation from Socrates in the hopes of getting out of financial straits. For our purposes, we don't need to concern ourselves as much with the plot as with the main characters and main ideas of the comedy. Or to put in in terms of an earlier experiment, we want to find the *key* words of *Clouds*, that is that words that refer to the characters, places, ideas, and so on that are specifically important to this play and so to, say, other plays of Aristophanes like *Lysistrata* or *The Frogs*. Our plan then in this experiment based on a chapter from the upcoming book *Exploratory Philology* is to generate a word cloud, a *Clouds* cloud if you will.\n",
    "\n",
    "Wordclouds are a text visualization that packs words densely into a confined space such that the words themselves are given visual priority—e.g. some words will be bigger than others—based on some criteria and often specifically based on frequency. As Hicke (2022) notes: \"Their intuitive and engaging design has made them a popular choice for authors, but many visualization specialists are skeptical about their effectiveness.\" Nevertheless, as Craig (2014) writes, the format can show students a \"systematic way to handle something many of them think of as ineffable\" and specifically one that can \"appeal to humanities students a way of questioning their intuitive approach to textual analysis.\"\n",
    "\n",
    "Before getting into the code, let's sketch out a plan for the experiment in pseudocode, i.e. some plain-language instructions that we can \"translate\" into working Python. Let's start with the following...\n",
    "\n",
    "1. Load the text of Aristophanes' *Clouds* and preprocess the text and create a Counter object with word frequencies. This will get us the data we need to create the wordcloud and give us a sense of what we should expect to see there.\n",
    "2. Import the `wordcloud` package and use it to create a word cloud from the Counter object.\n",
    "\n",
    "The thing is that we'll quickly realize that a wordcloud of just the most frequent words isn't very interesting. So we'll add a few more steps to get the *key* words in the text. We already know from previous experiments that καί and δέ and so on are going to be the most frequent words. But we also know that καί and δέ and de are not the main characters, so to speak, if *Clouds*. Our visualization should the literal main character like Socrates in big type. The Phrontisterion is central to the action of *Clouds* but doesn't appear in the other plays—it should call attention to itself in the mass of words. This play is about thinking and learning. The cloud should reflect this. So, we push the experiment one step further...\n",
    "\n",
    "3. Compute tf-idf scores for each word in all of the plays of Aristophanes.\n",
    "4. Create a word cloud based on the key words from *Clouds* (in comparison with the other plays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from cltkreaders.grc import GreekTesseraeCorpusReader\n",
    "from eptools import preprocess\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from tabulate import tabulate\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [CLTK Readers](https://github.com/diyclassics/cltk_readers) to get the text of Aristophanes' *Clouds* into memory and preprocess it in a way that is easier to visualize in a wordcloud, specifically we will lowercase the text and remove diacritics such as breathings and accents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode 1a: \"Load the text of Aristophanes' *Clouds*...\"\n",
    "\n",
    "CR = GreekTesseraeCorpusReader()\n",
    "\n",
    "file = [file for file in CR.fileids() if 'clouds' in file][0]\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sample from text\n",
    "\n",
    "text = next(CR.texts(file))\n",
    "print(text[:106])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode 1b: \"preprocess the text...\"\n",
    "\n",
    "text = preprocess(text, diacriticals=False)\n",
    "print(text[:84])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making the wordcloud, let's get a preliminary sense of which words are most frequent in the *Clouds*, especially since, without any further processing, the words that we should expect to see in the wordcloud are going to be these very words. A century ago, to get sense of word frequency you might look to a concordance like Dunbar (1883). With the digitized text, we can count words directly from the textual data using `Counter` from the `collections` module..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode 1c: \"... and create a Counter object with word frequencies.\"\n",
    "\n",
    "wordcount = Counter(text.split())\n",
    "print(tabulate(wordcount.most_common(10), headers=['Word', 'Count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list gives us a nice tabular view of the most frequent words in the text, but how can we get a better sense of the relative frequency of different words (and do it with a fully graphical representation)? For this, we turn to the wordcloud..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pseudocode 2: Import the `wordcloud` package and use it to create a word cloud from the Counter object.\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(regexp=r'\\b[^\\W\\d_]+\\b').generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the wordcloud\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear');\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can adjust the parameters, spec. the `max_font_size` parameter in the `WordCloud` class to cram even more words into the same space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the wordcloud, with lower `max_font_size`\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this frequency-based wordcloud, we see exactly the words we expected to see based on the Counter from above: και, τι, τον, γαρ, την, αλλ, ω, δ, αν, των. All important, of course, but carrying no signature of *Clouds*. It could be any play of Aristophanes, any work on Ancient Greek.\n",
    "\n",
    "Luckily we have ways of getting key terms based on a collection of documents, for example tf-idf. A full discussion of tf-idf—or term frequency-inverse document frequency—is out-of-scope for this blog post (though I do have a full tf-idf blog-post explainer coming soon!). For now, let's take the basic [Wikipedia definition](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) as our starting point:  we are talking about a \"measure of importance of a word to a document in a collection or corpus, adjusted for the fact that some words appear more frequently in general.\" The word for \"clouds\" appears more often *Clouds* than in the other Aristophanic works. Ditto \"frogs\" and *Frogs*. And the ways in which some words appear not merely frequently throughout a collection of documents but within specific documents (and not other!) can be measured using tf-idf. \n",
    "\n",
    "Let's use the approach here, using all of the plays of Aristophanes as ground to *Clouds* figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the files of Aristophanes' plays\n",
    "\n",
    "files = [file for file in CR.fileids() if 'aristophanes.' in file]\n",
    "pprint(files)\n",
    "print()\n",
    "\n",
    "# Make labels based on the files\n",
    "labels = [file.split('.')[1] for file in files]\n",
    "pprint(labels)\n",
    "print()\n",
    "\n",
    "texts = [preprocess(next(CR.texts(file)), diacriticals=False) for file in files]\n",
    "print(f'There are {len(texts)} texts in this collection.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pseudocode 3. Compute the tf-idf scores for each word in all of the plays of Aristophanes.\n",
    "\n",
    "# In fact we will build a document-term matrix (DTM) with the `CountVectorizer` class from `scikit-learn` first\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CV = CountVectorizer(input='content', max_df=.90)\n",
    "dtm = CV.fit_transform(texts)\n",
    "vocab = CV.get_feature_names_out()\n",
    "\n",
    "# Make dataframe\n",
    "import pandas as pd\n",
    "df_cv = pd.DataFrame(dtm.toarray(), columns=vocab, index=labels)\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a document-term matrix (DTM) with word counts for each play of Aristophanes. This kind of easy to navigate (if exceptionally large) data structure allows us to see quickly that αβατοισιν and αβατον appears once each in *Lysistrata*, that αβελτερον once in *Ecclesiazusae*. There are 11 rows each represent a play and there are 22,312 columns representing our \"vocabulary\".\n",
    "\n",
    "(It is not a complete vocabulary. Note the parameter `max_df=.90` which tells CountVectorizer to ignore words that appear in more than 90% of the documents. This helps remove very high frequency words, not unlike the ones we tended to see in the first batch of wordclouds we generated.)\n",
    "\n",
    "But while this DTM is a convenient data structure for representing—and comparing—documents generally, it is for us here a means to a tf-idf end. Remember from the definition of above that we want to use tf-idf as a way to adjust \"for the fact that some words appear more frequently in general\" or conversely that some words are more informative within certain documents than within others.\" \n",
    "\n",
    " This counts-based DTM can be easily transformed into a tf-idf matrix using the `TfidfTransformer` class from `sklearn`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the tfidf scores\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "TT = TfidfTransformer(use_idf=True).fit(dtm)\n",
    "tfidf_dtm = TT.transform(dtm)\n",
    "df_tfidf = pd.DataFrame(tfidf_dtm.toarray(), columns=vocab, index=labels)\n",
    "df_tfidf.iloc[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode 4. Create a word cloud based on the key words from *Clouds* (in comparison with the other plays).\n",
    "\n",
    "# Subset the dataframe to just the *Clouds* row\n",
    "clouds = df_tfidf.loc['clouds']\n",
    "clouds.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see some encouraging signs in the tf-idf scores that our keyness-based wordcloud will tell us something about *Clouds* and not, say, *Frogs*. We see Socrates. We see the Phrontisterion. A verb of learning tops the list. Perhaps most key of all, we see clouds, or should I say Clouds. (Note how the proper-name character of the Clouds as chorus has been obscured through preprocessing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pseudocode 4. Create a word cloud based on the key words from *Clouds* (in comparison with the other plays). (cont.)\n",
    "\n",
    "Cloud = WordCloud(background_color=\"white\", max_words=50).generate_from_frequencies(clouds)\n",
    "\n",
    "# Display the generated image\n",
    "plt.imshow(Cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do next? The clearest next step for this exploration would be to compare the *Cloud* wordclouds to those for the other Aristophanic works. They are already in memory, ready to be generated. But it is worth asking then—what does it mean to compare one wordcloud to another? We should also check how the wordcloud change when you look at *Clouds* against a different set of reference texts, say, e.g., the plays of Euripides? all of Greek poetry? etc. This is the kind of read-write-refactor coding style that is at the heart of *Exploratory Philology*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- Craig, Kalani. 2014. “Teaching Digital Humanities with Analog Tools: Word Clouds and Text Mining.” https://kalanicraig.com/teaching/teaching-digital-humanities-with-analog-tools-word-clouds-and-text-mining/.\n",
    "- Dunbar, Henry. 1883. *A Complete Concordance to the Comedies and Fragments of Aristophanes*. Oxford: Claredon Press.\n",
    "- Hicke, Rebecca M. M., Maanya Goenka, and Eric Alexander. 2022. “Word Clouds in the Wild.” In *2022 IEEE 7th Workshop on Visualization for the Digital Humanities (VIS4DH)*. Oklahoma City, OK: IEEE. https://doi.org/10.1109/VIS4DH57440.2022.00015: 43-48.\n",
    "- Mueller, Andreas C. 2020. “Wordcloud.” https://github.com/amueller/wordcloud."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit ('exploratory-blog')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cfcea2cd4d375f5df29dea34308e490dbf87598afedd7a9dce8ea7841d1a117"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
