[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Data science + Latin literature"
  },
  {
    "objectID": "posts/words-clouds/index.html",
    "href": "posts/words-clouds/index.html",
    "title": "Aristophanic Wordclouds",
    "section": "",
    "text": "In Aristophanes’s Clouds, we find a certain Strepsiades who joins a philosophy-rhetoric school—The Phrontisterion—in order to learn the arts of Strong and Weak Argumentation from Socrates in the hopes of getting out of financial straits. For our purposes, we don’t need to concern ourselves as much with the plot as with the main characters and main ideas of the comedy. In other words, we want to find the key words of Clouds, that is that words that refer to the characters, places, ideas, and so on that are specifically important to this play and not to, say, other plays of Aristophanes like Lysistrata or The Frogs. Our plan then in this experiment based on a chapter from the upcoming book Exploratory Philology is to generate a word cloud, a Clouds cloud if you will.\n\nRun this notebook in the browser using Binder here \nWordclouds are a text visualization that packs words densely into a confined space such that the words themselves are given visual priority—e.g. some words will be bigger than others—based on some criteria and often specifically based on frequency. As Hicke, Goenka, and Alexander (2022) note: “Their intuitive and engaging design has made them a popular choice for authors, but many visualization specialists are skeptical about their effectiveness.” Nevertheless, as Craig (2014) writes, the format can show students a “systematic way to handle something many of them think of as ineffable” and specifically one that can “appeal to humanities students a way of questioning their intuitive approach to textual analysis.”\nBefore getting into the code, let’s sketch out a plan for the experiment in pseudocode, i.e. some plain-language instructions that we can “translate” into working Python. Let’s start with the following…\nPseudocode for creating wordclouds for Aristophanes’ Clouds\n\nLoad the text of Aristophanes’ Clouds and preprocess the text and create a Counter object with word frequencies. This will get us the data we need to create the wordcloud and give us a sense of what we should expect to see there.\nImport the wordcloud package (Mueller (2020)) and use it to create a word cloud from the Counter object.\n\nThe thing is that we’ll quickly realize that a wordcloud of just the most frequent words isn’t very interesting. So we’ll add a few more steps to get the key words in the text. We already know from previous experiments that καί and δέ and so on are going to be the most frequent words. But we also know that καί and δέ are not the main characters, so to speak, of Clouds. Our visualization should have literal main characters like Socrates in big type. The Phronisterion is central to the action of Clouds but doesn’t appear in the other plays—it should call attention to itself in the mass of words. This play is about thinking and learning. The cloud should reflect this. So, we push the experiment one step further…\n\nCompute tf-idf scores for each word in all of the plays of Aristophanes.\nCreate a word cloud based on the key words from Clouds (in comparison with the other plays).\n\n\n# Imports\n\nfrom cltkreaders.grc import GreekTesseraeCorpusReader\nfrom eptools import preprocess\n\nfrom collections import Counter\n\nfrom tabulate import tabulate\nfrom pprint import pprint\n\nimport matplotlib.pyplot as plt\n\nWe’ll use CLTK Readers to get the text of Aristophanes’ Clouds into memory and preprocess it in a way that is easier to visualize in a wordcloud, specifically we will lowercase the text and remove diacritics such as breathings and accents.\n\n# Pseudocode 1a: \"Load the text of Aristophanes' *Clouds*...\"\n\nCR = GreekTesseraeCorpusReader()\n\nfile = [file for file in CR.fileids() if 'clouds' in file][0]\nprint(file)\n\naristophanes.clouds.tess\n\n\n\n# Print sample from text\n\ntext = next(CR.texts(file))\nprint(text[:106])\n\nἰοὺ ἰού:\nὦ Ζεῦ βασιλεῦ τὸ χρῆμα τῶν νυκτῶν ὅσον:\nἀπέραντον. οὐδέποθ' ἡμέρα γενήσεται;\n\n\n\n# Pseudocode 1b: \"preprocess the text...\"\n\ntext = preprocess(text, diacriticals=False)\nprint(text[:84])\n\nιου ιου \nω ζευ βασιλευ το χρημα των νυκτων οσον \nαπεραντον ουδεποθ ημερα γενησεται \n\n\n\nBefore making the wordcloud, let’s get a preliminary sense of which words are most frequent in the Clouds, especially since, without any further processing, the words that we should expect to see in the wordcloud are going to be these very words. A century ago, to get sense of word frequency you might look to a concordance like Dunbar (1883). With the digitized text, we can count words directly from the textual data using Counter from the collections module…\n\n# Pseudocode 1c: \"... and create a Counter object with word frequencies.\"\n\nwordcount = Counter(text.split())\nprint(tabulate(wordcount.most_common(10), headers=['Word', 'Count']))\n\nWord      Count\n------  -------\nκαι         276\nτι          170\nτον         153\nγαρ         106\nτην         104\nαλλ         101\nω            90\nδ            85\nαν           84\nτων          71\n\n\nThis list gives us a nice tabular view of the most frequent words in the text, but how can we get a better sense of the relative frequency of different words (and do it with a fully graphical representation)? For this, we turn to the wordcloud…\n\n#Pseudocode 2: Import the `wordcloud` package and use it to create a word cloud from the Counter object.\n\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(regexp=r'\\b[^\\W\\d_]+\\b').generate(text)\n\n\n# Display the wordcloud\n\nplt.imshow(wordcloud, interpolation='bilinear');\nplt.axis(\"off\");\n\n\n\n\nWe can adjust the parameters, spec. the max_font_size parameter in the WordCloud class to cram even more words into the same space.\n\n# Display the wordcloud, with lower `max_font_size`\n\nwordcloud = WordCloud(max_font_size=40).generate(text)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\nLooking at this frequency-based wordcloud, we see exactly the words we expected to see based on the Counter from above: και, τι, τον, γαρ, την, αλλ, ω, δ, αν, των. All important, of course, but carrying no signature of Clouds. It could be any play of Aristophanes, any work on Ancient Greek.\nLuckily we have ways of getting key terms based on a collection of documents, for example tf-idf. A full discussion of tf-idf—or term frequency-inverse document frequency—is out-of-scope for this blog post (though I do have a full tf-idf blog-post explainer coming soon!). For now, let’s take the basic Wikipedia definition as our starting point: we are talking about a “measure of importance of a word to a document in a collection or corpus, adjusted for the fact that some words appear more frequently in general.” The word for “clouds” appears more often Clouds than in the other Aristophanic works. Ditto “frogs” and Frogs. And the ways in which some words appear not merely frequently throughout a collection of documents but within specific documents (and not other!) can be measured using tf-idf.\nLet’s use the approach here, using all of the plays of Aristophanes as ground to Clouds figure.\n\n# Get all of the files of Aristophanes' plays\n\nfiles = [file for file in CR.fileids() if 'aristophanes.' in file]\npprint(files)\nprint()\n\n# Make labels based on the files\nlabels = [file.split('.')[1] for file in files]\npprint(labels)\nprint()\n\ntexts = [preprocess(next(CR.texts(file)), diacriticals=False) for file in files]\nprint(f'There are {len(texts)} texts in this collection.')\n\n['aristophanes.acharnians.tess',\n 'aristophanes.birds.tess',\n 'aristophanes.clouds.tess',\n 'aristophanes.ecclesiazusae.tess',\n 'aristophanes.frogs.tess',\n 'aristophanes.knights.tess',\n 'aristophanes.lysistrata.tess',\n 'aristophanes.peace.tess',\n 'aristophanes.plutus.tess',\n 'aristophanes.thesmophoriazusae.tess',\n 'aristophanes.wasps.tess']\n\n['acharnians',\n 'birds',\n 'clouds',\n 'ecclesiazusae',\n 'frogs',\n 'knights',\n 'lysistrata',\n 'peace',\n 'plutus',\n 'thesmophoriazusae',\n 'wasps']\n\nThere are 11 texts in this collection.\n\n\n\n#Pseudocode 3. Compute the tf-idf scores for each word in all of the plays of Aristophanes.\n\n# In fact we will build a document-term matrix (DTM) with the `CountVectorizer` class from `scikit-learn` first\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nCV = CountVectorizer(input='content', max_df=.90)\ndtm = CV.fit_transform(texts)\nvocab = CV.get_feature_names_out()\n\n# Make dataframe\nimport pandas as pd\ndf_cv = pd.DataFrame(dtm.toarray(), columns=vocab, index=labels)\ndf_cv\n\n\n\n\n\n\n\n\nαβατοισιν\nαβατον\nαβελτεροι\nαβελτερον\nαβελτερωτατοι\nαβιωτον\nαβουλευσαμεν\nαβυσσον\nαγ\nαγαγειν\n...\nωχοντο\nωχου\nωχρα\nωχραν\nωχριας\nωχριωντας\nωχριωσα\nωχρον\nωχρος\nωψωνηκ\n\n\n\n\nacharnians\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nbirds\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nclouds\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n1\n0\n1\n0\n1\n0\n0\n\n\necclesiazusae\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n...\n1\n2\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nfrogs\n0\n0\n0\n0\n1\n0\n0\n1\n1\n1\n...\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nknights\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nlysistrata\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\npeace\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nplutus\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\nthesmophoriazusae\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nwasps\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n11 rows × 22312 columns\n\n\n\nHere we have a document-term matrix (DTM) with word counts for each play of Aristophanes. This kind of easy to navigate (if exceptionally large) data structure allows us to see quickly that αβατοισιν and αβατον appears once each in Lysistrata, that αβελτερον once in Ecclesiazusae. There are 11 rows each represent a play and there are 22,312 columns representing our “vocabulary”.\n(It is not a complete vocabulary. Note the parameter max_df=.90 which tells CountVectorizer to ignore words that appear in more than 90% of the documents. This helps remove very high frequency words, not unlike the ones we tended to see in the first batch of wordclouds we generated.)\nBut while this DTM is a convenient data structure for representing—and comparing—documents generally, it is for us here a means to a tf-idf end. Remember from the definition of above that we want to use tf-idf as a way to adjust “for the fact that some words appear more frequently in general” or conversely that some words are more informative within certain documents than within others.”\nThis counts-based DTM can be easily transformed into a tf-idf matrix using the TfidfTransformer class from sklearn…\n\n# Now the tfidf scores\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nTT = TfidfTransformer(use_idf=True).fit(dtm)\ntfidf_dtm = TT.transform(dtm)\ndf_tfidf = pd.DataFrame(tfidf_dtm.toarray(), columns=vocab, index=labels)\ndf_tfidf.iloc[:, :5]\n\n\n\n\n\n\n\n\nαβατοισιν\nαβατον\nαβελτεροι\nαβελτερον\nαβελτερωτατοι\n\n\n\n\nacharnians\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nbirds\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nclouds\n0.000000\n0.000000\n0.014119\n0.000000\n0.000000\n\n\necclesiazusae\n0.000000\n0.000000\n0.000000\n0.017929\n0.000000\n\n\nfrogs\n0.000000\n0.000000\n0.000000\n0.000000\n0.013047\n\n\nknights\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nlysistrata\n0.015528\n0.015528\n0.000000\n0.000000\n0.000000\n\n\npeace\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nplutus\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nthesmophoriazusae\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nwasps\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n# Pseudocode 4. Create a word cloud based on the key words from *Clouds* (in comparison with the other plays).\n\n# Subset the dataframe to just the *Clouds* row\nclouds = df_tfidf.loc['clouds']\nclouds.sort_values(ascending=False).head(10)\n\nμανθανειν        0.127072\nνεφελαι          0.112953\nσωκρατες         0.098834\nθεους            0.095672\nυιον             0.094850\nφροντιστηριον    0.084714\nαρρενα           0.084714\nλογοιν           0.070595\nδινος            0.070595\nαυται            0.070050\nName: clouds, dtype: float64\n\n\nWe can already see some encouraging signs in the tf-idf scores that our keyness-based wordcloud will tell us something about Clouds and not, say, Frogs. We see Socrates. We see the Phrontisterion. A verb of learning tops the list. Perhaps most key of all, we see clouds, or should I say Clouds. (Note how the proper-name character of the Clouds as chorus has been obscured through preprocessing.)\n\n#Pseudocode 4. Create a word cloud based on the key words from *Clouds* (in comparison with the other plays). (cont.)\n\nCloud = WordCloud(background_color=\"white\", max_words=50).generate_from_frequencies(clouds)\n\n# Display the generated image\nplt.imshow(Cloud, interpolation='bilinear')\nplt.axis(\"off\");\n\n\n\n\nWhat to do next? The clearest next step for this exploration would be to compare the Cloud wordclouds to those for the other Aristophanic works. They are already in memory, ready to be generated. But it is worth asking then—what does it mean to compare one wordcloud to another? We should also check how the wordcloud change when you look at Clouds against a different set of reference texts, say, e.g., the plays of Euripides? all of Greek poetry? etc. This is the kind of read-write-refactor coding style that is at the heart of Exploratory Philology.\n\nReferences\n\n\nCraig, Kalani. 2014. “Teaching Digital Humanities with Analog Tools: Word Clouds and Text Mining.” https://kalanicraig.com/teaching/teaching-digital-humanities-with-analog-tools-word-clouds-and-text-mining/.\n\n\nDunbar, Henry. 1883. A Complete Concordance to the Comedies and Fragments of Aristophanes. Oxford: Claredon Press.\n\n\nHicke, Rebecca M. M., Maanya Goenka, and Eric Alexander. 2022. “Word Clouds in the Wild.” In 2022 IEEE 7th Workshop on Visualization for the Digital Humanities (VIS4DH), 43–48. Oklahoma City, OK, USA: IEEE. https://doi.org/10.1109/VIS4DH57440.2022.00015.\n\n\nMueller, Andreas C. 2020. “Wordcloud.” https://github.com/amueller/wordcloud."
  },
  {
    "objectID": "posts/virgilian-numbers/index.html",
    "href": "posts/virgilian-numbers/index.html",
    "title": "Virgilian Numbers",
    "section": "",
    "text": "How numerous are numbers in Latin poetry? It is one of those questions that a reader may have a vague sense of through the act of reading, but which would be difficult to quantify through reading alone. Surely you could read a Latin poem keeping a numeric scorecard as work through the text. And, in fact, that is exactly what we will do, or more precisely what we will task the computer with doing, since this is the kind of the “reading” at which the computer excels. So, for this experiment, let’s build a number counter that reads the works of Virgil—the Eclogues, the Georgics, the Aeneid—and keeps a tally of numbers that are encountered in each texts. We will then use the results to offer a provisional answer to the question—which of Virgil’s works is the most “numerical”?\n\nRun this notebook in the browser using Binder here \nIs this a good research question? It is surely not a completely uninteresting one. But what I want you to keep in mind as we develop this exercise—it is not so much that we are interested in “numericalness” as much as we are interested in description in general. How do we go about describing the texts we read? And how can computers help with this?\nSure we have some familiar ways of describing texts, e.g. genre. The Aeneid is an epic poem. The Eclogues is a collection of bucolic poetry. The Georgics are… well, sometimes I’m not sure I really know what the Georgics are, but they are certainly some sort of didactic poem. But once we are able to discern the formal features of texts, identify them, collect and count them, we can describe any number of literary phenomena. I could have just as easily chosen “animalness” in Virgil or “colorfulness” in Virgil. (We will in fact look at color in a later post.) Here we start with numbers and we do so for a specific reason. Numbers—for the most part, and for our purposes in this experiment, entirely so—are indeclinable. That means, at least at first, that we can restrict our attention to the wordform as it appears in the text, i.e. the token, and not to its dictionary definition, i.e. the lemma. We will though by the end of the post move on to working with lemmas.\nOur plan then in this experiment based on another chapter—see another chapter preview here—from the upcoming book Exploratory Philology is to generate a determine a kind of Virgilian “numericalness.”\nSo, what do we need to do to determine this “numericalness”? Before we write out any Python code, we turn first to pseudocode, i.e. a plain language description of the steps we will take to solve our problem. As I see the problem given above, we need to do the following…\nPseudocode for counting numbers in Virgil\n\nLoad our library of Latin texts, keeping only those by Virgil\nCreate a list of words that we will consider numbers\nFor each text by Virgil…\n\nRead the text into memory\nCount the words in the text that are also in the our number list\nStore the count of numbers in the text\n\nCreate a table of number counts\n\nAs you will see below, we will qualify how we present this table of counts, spec. normalizing the counts by text length. But for now we our pseudocode provides an excellent starting point for proceeding with our Describe experiment.\n\n# Preliminary imports\n\nfrom natsort import natsorted\nfrom pprint import pprint\nfrom time import sleep\n\nAs always, let’s set up our corpus reader and pull out the texts we want to describe.\n\n# PC 1: Load our library of Latin texts, keeping only those by Virgil\n\nfrom cltkreaders.lat import LatinTesseraeCorpusReader\n\nCR = LatinTesseraeCorpusReader()\n\neclogues = natsorted(CR.fileids(match='eclogues'))\ngeorgics = natsorted(CR.fileids(match='georgics'))\naeneid = natsorted(CR.fileids(match='aeneid'))\n\nIn order to find items in our texts, we will test for inclusion using the in operator. For example, we can check whether the number ‘seven’, i.e. septem, is in the text of the Eclogues by using the following code…\n\neclogues_words = [word for word in list(CR.words(eclogues, plaintext=True))]\npprint('septem' in eclogues_words)\n\nTrue\n\n\nIt is! And since this word in both indeclinable and unambiguous with respect to lemma (i.e. there is no other Latin word that has an oblique form septem), we can be confident that we have found what we are looking for. Same for octo…\n\nprint('octo' in eclogues_words)\n\nFalse\n\n\nThe word octo does not appear in the Eclogues. Again, we make this determination by checking for inclusion with the in operator. In order to get to where we want to go with this experiment, all we need to do is scale up our process—check more words against more texts, keeping track of what we find along the way. Let’s continue with the Eclogues and, instead of checking individual numbers, we will loop over a list of numbers, checking each one against the text. As noted above, we will skip the “declinable” numbers for now and limit ourselves to the numbers four through ten, i.e. our “indeclinables.”\nFirst, we make a list of numbers…\n\n# PC 2: Create a list of words that we will consider numbers\n\nnumbers = ['quattuor', 'quinque', 'sex', 'septem', 'octo', 'nouem', 'decem']\n\n…and then loop over this list. For now, we will just print True when the number is encountered and False when it is not…\n\nfor number in numbers:\n    print('--------------------')\n    print(f\"Checking '{number}' in eclogues_words...\")\n    if number in eclogues_words:\n        print(f'Found {number}!')\n    else:\n        print(f'Not found.')\n    print('--------------------')\n    sleep(1)\n\n--------------------\nChecking 'quattuor' in eclogues_words...\nFound quattuor!\n--------------------\n--------------------\nChecking 'quinque' in eclogues_words...\nNot found.\n--------------------\n--------------------\nChecking 'sex' in eclogues_words...\nNot found.\n--------------------\n--------------------\nChecking 'septem' in eclogues_words...\nFound septem!\n--------------------\n--------------------\nChecking 'octo' in eclogues_words...\nNot found.\n--------------------\n--------------------\nChecking 'nouem' in eclogues_words...\nNot found.\n--------------------\n--------------------\nChecking 'decem' in eclogues_words...\nFound decem!\n--------------------\n\n\nSurely, though it would be better to keep track of, not only whether a word is seen or not, but also of how many times we see it. We can do this easily enough by using a data structure specifically designed for this task, namely Counter. Counter is a dictionary-like structure in which keys, i.e. here the number words, are mapped to values, i.e. the number of times the key is seen. We just need to increment the count, i.e. add one to the existing value, every time a new instance of the number word is seen. We will once again use the in operator to check for inclusion, but note that this time we reverse the test—instead of checking whether the number word is in the text, we check whether each word in the text is in the list of number words.\n\n# PC 3a & b: \n#    - Count the words in the text that are also in the our number list\n#    - Store the count of numbers in the text\n\nfrom collections import Counter\n\nC = Counter()\n\nfor word in eclogues_words:\n    if word in numbers:\n        C[word] += 1\n\nprint(C)\n\nCounter({'decem': 2, 'septem': 1, 'quattuor': 1})\n\n\nThat is not a lot of number words. Again, think back to the beginning of this post—as a reader of the Eclogues you may intuit that numbers just do not come up often. But could you have read the ten poems start to finish and announced that you had seen the number septem only once? Probably not.\nOne of the most empowering things about computational approaches to philological problems is the flexibility that comes from simple refactoring. With a change of very little, only a line or two of code, we can switch easily from the “numericalness” of the Eclogues to that of the Georgics…\n\ngeorgics_words = list(CR.words(georgics, plaintext=True))\n\nAnd let’s just get the Aeneid words out of the way now too…\n\naeneid_words = list(CR.words(aeneid, plaintext=True))\n\nCounter works pretty fast and—in the interest of showing the many different ways to approach a philological problem—let’s arrive at our number counts in a slightly different way this time. Let’s build a counter for each poem with all of the words and then only keep the counts of interest, i.e. the number words. We do this not with a list comprehension, like e.g. we have seen when working with our file list, but rather a dictionary comprehension.\nIt works like so: we loop over the numbers list and make each number a key in the dictionary while assigning that key the value from the complete word count. A nice trick and an efficient one—it takes no more time to count everything and select from what we have than to loop over everything and add what we want and it is likely faster. Loops have their place in coding and we will use them often. But we should recognize too that there are often more efficient ways to approach problems. We will see this over and over again in exploratory philological work.\n\neclogues_C = Counter(eclogues_words)\ngeorgics_C = Counter(georgics_words)\naeneid_C = Counter(aeneid_words)\n\neclogues_nums_C = {k: eclogues_C[k] for k in numbers}\ngeorgics_nums_C = {k: georgics_C[k] for k in numbers}\naeneid_nums_C = {k: aeneid_C[k] for k in numbers}\n\n# Print an example\nprint(georgics_nums_C)\n\n{'quattuor': 6, 'quinque': 0, 'sex': 0, 'septem': 3, 'octo': 1, 'nouem': 0, 'decem': 1}\n\n\nWe know have three Counters with “numericalness” data from each of Virgil’s works. Let’s present them in a tabular format so that we can compare them.\n\n# PC 4: Create a table of number counts\n\nimport pandas as pd\n\ndf = pd.DataFrame([eclogues_nums_C, georgics_nums_C, aeneid_nums_C], index=['Eclogues', 'Georgics', 'Aeneid'])\ndf\n\n\n\n\n\n\n\n\nquattuor\nquinque\nsex\nseptem\nocto\nnouem\ndecem\n\n\n\n\nEclogues\n1\n0\n0\n1\n0\n0\n2\n\n\nGeorgics\n6\n0\n0\n3\n1\n0\n1\n\n\nAeneid\n7\n1\n4\n12\n0\n0\n2\n\n\n\n\n\n\n\nThe Aeneid has more numbers than the other two works—at least, more of the numbers 4 through 10. But this twelve-book epic poem is also much longer than the other works. Is this a fair comparison? Not at all. So let’s take the added step of normalizing the existing counts. We do this by dividing each count by the number of words in the corresponding text. To make things more readable—these are afterall very low wordcounts—we multiply the result by 1000 and to make things even more readable we can round this number to two decimal places. What we have then is a count per 1000 words which we can use as a basis for comparison.\nFirst, we need whole-work word counts. This is all straightforward enough, though putting it all together compactly verges towards an advance topic. Here is how you could approach it—don’t worry if all of the details are not clear yet. We will explore Pandas in greater depth in subsequent posts.\n\neclogues_words_total = len(eclogues_words)\ngeorgics_words_total = len(georgics_words)\naeneid_words_total = len(aeneid_words)\n\n# Print an example\nprint(f'There are {eclogues_words_total} words in the Eclogues.')\n\nThere are 7201 words in the Eclogues.\n\n\n\ncounts = [eclogues_words_total, georgics_words_total, aeneid_words_total]\n\ndf_norm = df.div(counts, axis='rows').mul(1000).round(2)\ndf_norm\n\n\n\n\n\n\n\n\nquattuor\nquinque\nsex\nseptem\nocto\nnouem\ndecem\n\n\n\n\nEclogues\n0.14\n0.00\n0.00\n0.14\n0.00\n0.0\n0.28\n\n\nGeorgics\n0.36\n0.00\n0.00\n0.18\n0.06\n0.0\n0.06\n\n\nAeneid\n0.09\n0.01\n0.05\n0.15\n0.00\n0.0\n0.03\n\n\n\n\n\n\n\nBut what we really want is a total of our words of interest over the total word count…\n\ndf_nums = df.sum(axis=1)\ndf_nums\n\nEclogues     4\nGeorgics    11\nAeneid      26\ndtype: int64\n\n\n\ndf_nums.div(counts, axis='rows').mul(1000).round(3)\n\nEclogues    0.555\nGeorgics    0.652\nAeneid      0.325\ndtype: float64\n\n\nJust to confirm that we understand how Pandas arrives at this number for the Eclogues…\n\nprint(f'Step 1: 4 number words / {eclogues_words_total} total words = {4/7129}')\nprint(f'Step 2: ( 4 number words / {eclogues_words_total} total words ) * 1000 = {(4/7129) * 1000}')\nprint(f'Step 3: \"        \"        \"    rounded to the third decimal place = {round((4/7129) * 1000, 3)}')\n\nStep 1: 4 number words / 7201 total words = 0.0005610885117127227\nStep 2: ( 4 number words / 7201 total words ) * 1000 = 0.5610885117127227\nStep 3: \"        \"        \"    rounded to the third decimal place = 0.561\n\n\nSo while the Aeneid has more number words in absolute terms that the other two works, in relative terms it has fewer. And the Georgics edges out the Eclogues—at least as we defined the problem—i.e. as a count of the indeclinable numbers quattuor through decem. Let’s see what we can do now with lemmas and not just tokens.\nWe will use a data structure available to us in the Tesserae CorpusReader called tokenized_sents which generates one sentence at a time in the format of a list of length-three tuples of the form (token, lemma, POS tag). Here is an example from the beginning of the Eclogues…\n\neclogues_tokenized_sents = CR.tokenized_sents(eclogues)\n\npprint(next(eclogues_tokenized_sents)[:5])\n\n[('Tityre', 'tityrus', 'PROPN'),\n (',', ',', 'PUNCT'),\n ('tu', 'tu', 'PRON'),\n ('patulae', 'patula', 'NOUN'),\n ('recubans', 'recubo', 'VERB')]\n\n\nWe can now increase our list of numbers now to include the ‘declinables’, i.e. the Latin words for the numbers one through three. And we can do so using lemmas, since we now have access to that information through the tokenized_sents. Having increased the number list, we can iterate once again over our texts to get updated counts.\n\nnumbers = ['unus', 'duo', 'tres', 'quattuor', 'quinque', 'sex', 'septem', 'octo', 'nouem', 'decem']\n\neclogues_tokenized_sents = CR.tokenized_sents(eclogues)\n\neclogues_lemmas = []\n\nfor sent in eclogues_tokenized_sents:\n    for word, lemma, pos in sent:\n        eclogues_lemmas.append(lemma)\n\neclogues_C = Counter(eclogues_lemmas)\n\neclogues_nums_C = {k: eclogues_C[k] for k in numbers}        \n\n# Print an example\nprint(eclogues_nums_C)             \n\n{'unus': 5, 'duo': 6, 'tres': 2, 'quattuor': 1, 'quinque': 0, 'sex': 0, 'septem': 1, 'octo': 0, 'nouem': 0, 'decem': 2}\n\n\n\ngeorgics_tokenized_sents = CR.tokenized_sents(georgics)\n\ngeorgics_lemmas = []\n\nfor sent in georgics_tokenized_sents:\n    for word, lemma, pos in sent:\n        georgics_lemmas.append(lemma)\n\ngeorgics_C = Counter(georgics_lemmas)\n\ngeorgics_nums_C = {k: georgics_C[k] for k in numbers}\n\n# Print an example\nprint(georgics_nums_C)        \n\n{'unus': 12, 'duo': 5, 'tres': 2, 'quattuor': 6, 'quinque': 0, 'sex': 2, 'septem': 3, 'octo': 1, 'nouem': 0, 'decem': 1}\n\n\n\naeneid_tokenized_sents = CR.tokenized_sents(aeneid)\n\naeneid_lemmas = []\n\nfor sent in aeneid_tokenized_sents:\n    for word, lemma, pos in sent:\n        aeneid_lemmas.append(lemma)\n\naeneid_C = Counter(aeneid_lemmas)\n\naeneid_nums_C = {k: aeneid_C[k] for k in numbers}\n\n# Print an example\nprint(aeneid_nums_C)        \n\n{'unus': 113, 'duo': 23, 'tres': 11, 'quattuor': 7, 'quinque': 1, 'sex': 8, 'septem': 12, 'octo': 0, 'nouem': 4, 'decem': 2}\n\n\nWe combine these again into a dataframe of raw counts…\n\nlemma_df = pd.DataFrame([eclogues_nums_C, georgics_nums_C, aeneid_nums_C], index=['Eclogues', 'Georgics', 'Aeneid'])\nlemma_df_nums = lemma_df.sum(axis=1)\nlemma_df_nums\n\nEclogues     17\nGeorgics     32\nAeneid      181\ndtype: int64\n\n\n…and again normalize by total number of words in each text per 1000 words.\n\nlemma_df_nums_norm = lemma_df_nums.div(counts, axis='rows').mul(1000).round(2)\nlemma_df_nums_norm\n\nEclogues    2.36\nGeorgics    1.90\nAeneid      2.27\ndtype: float64\n\n\n\n# A quick visualization...\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nplt.figure(figsize=(4, 1))\nplt.rcParams['ytick.labelsize'] = 8\n\nax = lemma_df_nums_norm.plot(kind='barh', width=0.6)\nax.invert_yaxis()\nax.set(xlabel='Number of number words per 1000 words', title='\"Numericalness\" in Virgil');\n\n\n\n\nWith our increased wordlist—lemma list really—we see a shift in the results. The high number of unus instances shifts the Eclogues into the lead as the most numerical of the three works with the Georgics slipping to third. But these are only results, not conclusions. So many questions remain…\n\nDoes this match our prior expectation of Virgilian “numericalness” from our reading experience?\nIs the count of unus correct, i.e. how accurate is our lemmatizer? (see Error analysis below)\nWhat about numbers higher than 10? etc.\n\nBut while questions remain, it is also true that we have these questions because we were able with a reasonable amount of effort to engage in some exploratory textual data analysis with our texts. To apporach these questions with pen-and-paper would take hours; with Python, it takes minutes. This is a considerable extension of our philological toolkit.\nWhere to turn next? In Exploratory Philology, a great deal of attention is given to small acts of refactoring that open up wide avenues of potential further inquiry. A few examples based on this experiment…\n\nChange author: What if we wanted to compare the “numericalness” of Virgil’s works to those of another author? How would we do this? What other authors might we compare to Virgil? Perhaps we could try the experiment again with Ovid? or Lucan?\nChange concept: What if, instead of “numericalness, we want to capture a different concept? What if we wanted to compare the”animalness” of Virgil’s works? How might we approach this problem? (Hint: we could start with a single animal word, e.g. equus, and then expand to an increasingly longer list of animal words.) How are animals (or other concepts) different from numbers? Or, stated differently, could you ever compose a complete list of animal words?\nCompare against existing arguments: Bramble (1983) (following André (1949)) writes that Lucan’s use of color vocabulary is “less rich than that of mainstream epic” and that it is a “predominantly monchrome epic.” Does this bear itself out under further scrutiny? (It may be further helpful to consult Bramble’s loose statistical account of this phenomenon, e.g. does virens appear only once in Lucan (and specifically at 9.523? is it the only “green” word?) Or when we see that Bramble writes—“From a total of 34 terms, white, grey and black are the dominant tones, accounting for in terms with 64 occurrences.”—how can operationalize this loose definition of color terms into something that can be quantified using the methods above?\nError analysis: Our results are only as good as our data and the tools/methods applied to our data. So, it is important that we have a sense of the accuracy of what has been output to the tables above. E.g. the lemmatizer used in the tokenized_sents method above has a reported accuracy of 94.6%—good, but obviously not perfect. One place where we should already have some pause—for our word counts in the Aeneid, we have sex at 6 instances and in our lemmas counts we have 8 instances. Something is off—as a next step, it would be a good idea to open up the Aeneid texts and search and/or read through the file to see what may be happening. It is good to do this kind of error analysis periodically so that we are aware of what is working well and what isn’t in our philological explorations.\n\n\nReferences\n\n\nAndré, Jacques. 1949. Étude Sur Les Termes de Couleur Dans La Langue Latine. Paris: Librairie C. Klincksieck.\n\n\nBramble, J. C. 1983. “Lucan.” In Cambridge History Latin Literature: The Early Principate, edited by E. J. Kenney, 533–57. Cambridge: Cambridge University Press."
  },
  {
    "objectID": "posts/wills-1996/index.html",
    "href": "posts/wills-1996/index.html",
    "title": "Gemination (Wills 1996)",
    "section": "",
    "text": "In the first part of his 1996 monograph on repetition in Latin poetry, Jeffrey Wills (1996) discusses gemination. i.e. “the repetition of a word in the same form in the same clause with no additional expansion.” In this notebook, we will formalize Wills’ definition of gemination into code using LatinCy.\nLet’s start by setting up a code notebook with Python imports, etc. We will use CLTK Readers with the CLTK-Tesserae texts as our exploratory background for gemination.\n\n# Imports\n\nfrom collections import Counter\nfrom natsort import natsorted\n\nimport spacy\nfrom cltkreaders.lat import LatinTesseraeCorpusReader\n\nfrom latintools import preprocess\n\nfrom tabulate import tabulate\nfrom IPython.core.display import HTML\nfrom IPython.display import display\n\nfrom tqdm import tqdm\n\nWills uses the following line from Virgil’s Eclogues to illustrate gemination (V. Ecl. 2.69):\n\nCorydon, Corydon, quae te dementia cepit!\n\nLet’s begin there.\n\n# Set up corpus\n\nT = LatinTesseraeCorpusReader()\n\n# Get Eclogues file\n\neclogues = [file for file in T.fileids() if 'eclogues' in file][0]\nprint(eclogues)\n\nvergil.eclogues.tess\n\n\nNext we load a LatinCy model to assist with matching Latin wordforms.\n\n# Set up NLP\n\nnlp = spacy.load('la_core_web_lg')\n\n\nSimple gemination\nConsidering Wills’ basic definition from above, we can use the following pseudocode as a starting point…\n\nGet a line of Virgil\n\nCreate a LatinCy Doc for each line\n\nCount the norm token attributes for each line\n\nCheck norm count, i.e. if the count of norm token attributes is greater than 1, then the line has gemination\n\nNote that Wills specifically defines the scope of gemination as a clause (not a line); we will return to this point in a future notebook where we introduce some clause parsing.\nGet a line of Virgil\nFor the Tesserae texts, CLTK Readers has a data structure called doc_rows that, at least for poetry, gives us a dictionary with the format {citation: line, etc.}. Let’s get the docrows for the Eclogues and print a sample line.\n\n# Get all Eclogue rows\n\ndocrows = next(T.doc_rows(eclogues))\n\n\n# Get a row\n\ntest = docrows['&lt;verg. ecl. 2.69&gt;']\nprint(test)\n\nAh, Corydon, Corydon, quae te dementia cepit!\n\n\nWe can already see our gemination—specifically the example Wills uses in his defintion—with the repetition of Corydon.\nCreate a LatinCy Doc\nNext we can create a spaCy Doc for each line. The Doc contains all sorts of annotations useful for philological work. We will use the norm token attribute here to help us match wordforms.\n\n# Create LatinCy Doc for line\n\ndoc = nlp(test)\nprint(type(doc))\n\n&lt;class 'spacy.tokens.doc.Doc'&gt;\n\n\n\n# Print norm examples\nprint(tabulate([[token.i, token.text, token.norm_] for token in doc], headers=['Index','Token', 'Norm']))\n\n  Index  Token     Norm\n-------  --------  --------\n      0  Ah        ah\n      1  ,         ,\n      2  Corydon   corydon\n      3  ,         ,\n      4  Corydon   corydon\n      5  ,         ,\n      6  quae      quae\n      7  te        te\n      8  dementia  dementia\n      9  cepit     cepit\n     10  !         !\n\n\nThinking ahead, if we use lines as is from the Tesserae texts, we have to deal with punctuation. Wills is concerned with the repetition of Corydon, not the repetition of the commas! One way we can deal with this is to preprocess the lines to remove punctuation before creating the Docs. We will discuss the philological implications of preprocessing in a future notebook. For now, we are going to import a script called preprocess that removes punctuation.\n\n# Create LatinCy Doc for preprocessed line and print example\n\ndoc = nlp(preprocess(test, lower=False))\nprint(tabulate([[token.i, token.text, token.norm_] for token in doc], headers=['Index','Token', 'Norm']))\n\n  Index  Token     Norm\n-------  --------  --------\n      0  Ah        ah\n      1  Corydon   corydon\n      2  Corydon   corydon\n      3  quae      quae\n      4  te        te\n      5  dementia  dementia\n      6  cepit     cepit\n\n\nCount the norm token attributes\nWe can now count the norm token attributes for each line using a Counter from the collections module.\n\n# Count `norm` attr in Doc tokens\n\nnorms = [token.norm_ for token in doc]\nnorms_counter = Counter(norms)\nprint(norms_counter)\n\nCounter({'corydon': 2, 'ah': 1, 'quae': 1, 'te': 1, 'dementia': 1, 'cepit': 1})\n\n\nCheck norm count\nWe can now check the norm count for each line. If the count is greater than 1, then the line has gemination.\n\ngeminations = [k for k, v in norms_counter.items() if v &gt; 1]\nprint(f'Number of geminations: {len(geminations)}')\nprint(f'{geminations}')\n\nNumber of geminations: 1\n['corydon']\n\n\nWe knew from Wills that this line would have gemination; of course, not all lines do.\n\n# Try a different line\n\ntest = docrows['&lt;verg. ecl. 2.70&gt;']\ndoc = nlp(preprocess(test))\nnorms = [token.norm_ for token in doc]\nnorms_counter = Counter(norms)\ngeminations = [k for k, v in norms_counter.items() if v &gt; 1]\n\nprint(doc.text)\nprint(f'Number of geminations: {len(geminations)}')\nprint(f'{geminations}')\n\nsemiputata tibi frondosa uitis in ulmo est\nNumber of geminations: 0\n[]\n\n\nHaving worked through our pseudocode, we can now put it all together into a function that we can use to check for gemination in any line of Latin poetry.\n\ndef get_geminations(Doc):\n    norms = [token.norm_ for token in Doc]\n    norms_counter = Counter(norms)\n    geminations = [k for k, v in norms_counter.items() if v &gt; 1]\n    return geminations    \n\nUsing this function, we can loop through the docrows for the Eclogues and check for gemination in each line. In the example below, we break after the first match as we are only checking at this point that the function works as expected.\n\nfor k, v in docrows.items():\n    doc = nlp(preprocess(v, lower=False))\n    geminations = get_geminations(doc)\n    if len(geminations) &gt; 0:\n        print(f'{k}: {geminations}')\n        print(f'{v}')\n        print('\\n')\n        break\n\n&lt;verg. ecl. 1.23&gt;: ['sic']\nsic canibus catulos similis, sic matribus haedos\n\n\n\n\nMore useful of course would be to collect all of the geminations into a data structure like a dictionary…\n\nvirgil_geminations = {}\n\nfor k, v in tqdm(docrows.items()):\n    doc = nlp(preprocess(v))\n    geminations = get_geminations(doc)\n    if geminations:\n        virgil_geminations[k] = (v, geminations)\n\nprint(f'There are {len(virgil_geminations)} geminations in Virgil\\'s *Eclogues*.')\n\n100%|██████████| 828/828 [00:03&lt;00:00, 207.68it/s]\n\n\nThere are 105 geminations in Virgil's *Eclogues*.\n\n\n\nprint('Here are the first five examples from our search:\\n')\nfor k, v in list(virgil_geminations.items())[:5]:\n    print(f'{k}: {v[0]}')\n\nHere are the first five examples from our search:\n\n&lt;verg. ecl. 1.23&gt;: sic canibus catulos similis, sic matribus haedos\n&lt;verg. ecl. 1.33&gt;: nec spes libertatis erat, nec cura peculi:\n&lt;verg. ecl. 1.63&gt;: aut Ararim Parthus bibet, aut Germania Tigrim,\n&lt;verg. ecl. 1.75&gt;: Ite meae, felix quondam pecus, ite capellae.\n&lt;verg. ecl. 2.20&gt;: quam dives pecoris, nivei quam lactis abundans.\n\n\nNote V. Ecl. 1.75 as an example of why we use norm instead of text for matching wordforms. Ite is capitalized here only because it is the first word in the sentence, but should be matched against ite regardless of case. Note the following in Python string matching…\n\nprint('Ite' == 'ite')\nprint('ite' == 'ite')\n\nFalse\nTrue\n\n\nWe can make it easier to see gemination in our texts by formatting matched tokens in HTML. We can use the display module from the IPython package to display the HTML in the notebook.\n\ndef display_gemination(gemination):\n    html = ''\n    line = nlp(gemination[0])\n    terms = gemination[1]\n\n    for token in line:\n        if token.norm_ in terms:\n            token = f'&lt;span style=\"color: green;\"&gt;{token}&lt;/span&gt;'\n        html += f'{token} '\n    return html\n\n\nprint('Here are the first five examples from our search:')\nfor k, v in list(virgil_geminations.items())[:5]:\n    # Note that if you do not remove the angle brackets from the Tesserae citation, it will be ignored as a (bad) HTML tag in the formatting below.\n    citation = k.replace('&lt;', '').replace('&gt;', '') \n    citation = f'&lt;span style=\"color: black; font-weight: bold;\"&gt;{citation}&lt;/span&gt;'\n    text = display_gemination(v)\n    html = '&lt;br&gt;'.join([citation, text])\n    html += '&lt;br&gt;&lt;br&gt;'\n    display(HTML(html))\n\nHere are the first five examples from our search:\n\n\nverg. ecl. 1.23sic canibus catulos similis , sic matribus haedos \n\n\nverg. ecl. 1.33nec spes libertatis erat , nec cura peculi : \n\n\nverg. ecl. 1.63aut Ararim Parthus bibet , aut Germania Tigrim , \n\n\nverg. ecl. 1.75Ite meae , felix quondam pecus , ite capellae . \n\n\nverg. ecl. 2.20quam dives pecoris , nivei quam lactis abundans . \n\n\nMoreover, we can write these matches to a file, formatting the geminations to make them easier to spot, here wrapping repeitions with asterisks.\n\ndef format_gemination(gemination):\n    txt = ''\n    line = nlp(gemination[0])\n    terms = gemination[1]\n\n    for token in line:\n        if token.norm_ in terms:\n            token = f'*{token}*'\n        txt += f'{token} '\n    return txt\n\nwith open('eclogue_geminations.txt', 'w') as f:\n    for k, v in virgil_geminations.items():\n        citation = k.replace('&lt;', '').replace('&gt;', '')\n        citation = f'{citation}'\n        text = format_gemination(v)\n        f.write(f'{citation}\\t{text}\\n')\n\nNote that a line like Ecl. 4.51 appears in the output…\n\nterrasque tractusque maris caelumque profundum !\n\n…as que is considered a token in the LatinCy model.\nAccordingly, we may want to have a way to drop certain tokens from our matching process. We add below an exclude parameter to the get_geminations function to accomplish this.\n\ndef get_geminations(Doc, exclude=[]):\n    norms = [token.norm_ for token in Doc]\n    norms_counter = Counter(norms)\n    geminations = [k for k, v in norms_counter.items() if v &gt; 1 and k not in exclude]\n    return geminations    \n\nexclude =['que']\n\ntest = nlp(preprocess(docrows['&lt;verg. ecl. 4.51&gt;'], lower=False))\n\nprint('Before...')\nprint(get_geminations(test))\nprint()\nprint('After...')\nprint(get_geminations(test, exclude=exclude))\n\nBefore...\n['que']\n\nAfter...\n[]\n\n\nWe write to file again, this time excluding que.\n\nvirgil_geminations = {}\n\nfor k, v in tqdm(docrows.items()):\n    doc = nlp(preprocess(v))\n    geminations = get_geminations(doc, exclude=['que'])\n    if geminations:\n        virgil_geminations[k] = (v, geminations)\n\nwith open('eclogue_geminations.txt', 'w') as f:\n    for k, v in virgil_geminations.items():\n        citation = k.replace('&lt;', '').replace('&gt;', '')\n        citation = f'{citation}'\n        text = format_gemination(v)\n        f.write(f'{citation}\\t{text}\\n')\n\n100%|██████████| 828/828 [00:03&lt;00:00, 213.19it/s]\n\n\nSo far, we have worked only with the Eclogues. We could easily expand this gemination search to other texts in the Tesserae corpus. Here is an example of expanding it to all epic poems in the collection.\n\n# Geminations in all Latin epic\n\n# Note here I get the year from the Tesserae metadata, sort the files chronologically, and then discard the date information\nepic = natsorted([(file, int(T.metadata('date', file))) for file in T.fileids() if T.metadata('genre', file) == 'epic'], key=lambda x: x[1])\nepic = [file for file, _ in epic]\nprint(f'There are {len(epic)} epic poems in the Tesserae collection.')\n\nThere are 120 epic poems in the Tesserae collection.\n\n\n\n# This takes about 7 minutes on my laptop\n\nall_geminations = {}\n\nfor file in tqdm(epic):\n    docrows = next(T.doc_rows(file))\n    for k, v in docrows.items():\n        doc = nlp(preprocess(v))\n        geminations = get_geminations(doc, exclude=['que'])\n        if geminations:\n            all_geminations[k] = (v, geminations)\n\n100%|██████████| 120/120 [06:49&lt;00:00,  3.42s/it]\n\n\n\n# Write to file\nwith open('epic_geminations.tsv', 'w') as f:\n    f.write('citation\\ttext\\n')\n    for k, v in all_geminations.items():\n        citation = k.replace('&lt;', '').replace('&gt;', '')\n        citation = f'{citation}'\n        text = format_gemination(v)\n        f.write(f'{citation}\\t{text}\\n')\n\nThis has been an introduction to formalizing a literary critical/philological argument using LatinCy, an example that barely takes us past the first page of Wills Part I. In subsequent notebooks, we will explore variations on gemination and other types of repetition.\n\n\nReferences\n\n\nWills, Jeffrey. 1996. Repetition in Latin Poetry: Figures of Allusion. Clarendon Press."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploratory Philology Blog",
    "section": "",
    "text": "Virgilian Numbers\n\n\n\n\n\n\n\nexploratory-philology\n\n\nlatin\n\n\ntext-analysis\n\n\n\n\nAnother ‘Exploratory Philology’ code notebook showing how to count simple groups of words in Latin texts\n\n\n\n\n\n\nOct 20, 2023\n\n\nPatrick J. Burns\n\n\n\n\n\n\n  \n\n\n\n\nAristophanic Wordclouds\n\n\n\n\n\n\n\nexploratory-philology\n\n\nancient-greek\n\n\ntext-analysis\n\n\nvisualization\n\n\n\n\nAn ‘Exploratory Philology’ code notebook showing how to construct wordclouds of key words from Ancient Greek text\n\n\n\n\n\n\nOct 13, 2023\n\n\nPatrick J. Burns\n\n\n\n\n\n\n  \n\n\n\n\nGemination (Wills 1996)\n\n\n\n\n\n\n\nreplicating-classics\n\n\nlatin\n\n\ndata-science\n\n\nallusion\n\n\n\n\nCode notebook providing basic formalization for ‘gemination’ as defined in Wills 1996\n\n\n\n\n\n\nJun 28, 2023\n\n\nPatrick J. Burns\n\n\n\n\n\n\nNo matching items"
  }
]