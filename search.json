[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Data science + Latin literature"
  },
  {
    "objectID": "posts/words-clouds/index.html",
    "href": "posts/words-clouds/index.html",
    "title": "Aristophanic Wordclouds",
    "section": "",
    "text": "To run this notebook in the browser, you can use this Binder version.\n\n\n\nBinder\n\n\nIn Aristophanes’s Clouds, we find a certain Strepsiades who joins a philosophy-rhetoric school—The Phrontisterion—in order to learn the arts of Strong and Weak Argumentation from Socrates in the hopes of getting out of financial straits. For our purposes, we don’t need to concern ourselves as much with the plot as with the main characters and main ideas of the comedy. Or to put in in terms of an earlier experiment, we want to find the key words of Clouds, that is that words that refer to the characters, places, ideas, and so on that are specifically important to this play and so to, say, other plays of Aristophanes like Lysistrata or The Frogs. Our plan then in this experiment based on a chapter from the upcoming book Exploratory Philology is to generate a wordcloud, a Clouds cloud if you will.\n\nWordclouds are a text visualization that packs words densely into a confined space such that the words themselves are given visual priority—e.g. some words will be bigger than others—based on some criteria and often specifically based on frequency. As Hicke, Goenka, and Alexander (2022) notes: “Their intuitive and engaging design has made them a popular choice for authors, but many visualization specialists are skeptical about their effectiveness.” Nevertheless, as Craig (2014) writes, the format can show students a “systematic way to handle something many of them think of as ineffable” and specifically one that can “appeal to humanities students a way of questioning their intuitive approach to textual analysis.”\nBefore getting into the code, let’s sketch out a plan for the experiment in pseudocode, i.e. some plain-language instructions that we can “translate” into working Python. Let’s start with the following…\n\nLoad the text of Aristophanes’ Clouds and preprocess the text and create a Counter object with word frequencies. This will get us the data we need to create the wordcloud and give us a sense of what we should expect to see there.\nImport the wordcloud package (Mueller (2020)) and use it to create a wordcloud from the Counter object.\n\nThe thing is that we’ll quickly realize that a wordcloud of just the most frequent words isn’t very interesting. So we’ll add a few more steps to get the key words in the text. We already know from previous experiments that kai and de and so on are going to be the most frequent words. But we also know that καί and δέ and de are not the main characters, so to speak, if Clouds. Our visualization should the literal main character like Socrates in big type. The Phrontisterion is central to the action of Clouds but doesn’t appear in the other plays—it should call attention to itself in the mass of words. This play is about thinking and learning. The cloud should reflect this. So, we push the experiment one step further…\n\nCompute tf-idf scores for each word in all of the plays of Aristophanes.\nCreate a wordcloud based on the key words from Clouds (in comparison with the other plays).\n\n\n# Imports\n\nfrom cltkreaders.grc import GreekTesseraeCorpusReader\nfrom eptools import preprocess\n\nfrom collections import Counter\n\nfrom tabulate import tabulate\nfrom pprint import pprint\n\nimport matplotlib.pyplot as plt\n\nWe’ll use CLTK Readers to get the text of Aristophanes’ Clouds into memory and preprocess it in a way that is easier to visualize in a wordcloud, specifically we will lowercase the text and remove diacritics such as breathings and accents.\n\n# Pseudocode 1a: \"Load the text of Aristophanes' *Clouds*...\"\n\nCR = GreekTesseraeCorpusReader()\n\nfile = [file for file in CR.fileids() if 'clouds' in file][0]\nprint(file)\n\n\n# Print sample from text\n\ntext = next(CR.texts(file))\nprint(text[:106])\n\n\n# Pseudocode 1b: \"preprocess the text...\"\n\ntext = preprocess(text, diacriticals=False)\nprint(text[:84])\n\nBefore making the wordcloud, let’s get a preliminary sense of which words are most frequent in the Clouds, especially since, without any further processing, the words that we should expect to see in the wordcloud are going to be these very words. A century ago, to get sense of word frequency you might look to a concordance like Dunbar (1883). With the digitized text, we can count words directly from the textual data using Counter from the collections module…\n\n# Pseudocode 1c: \"... and create a Counter object with word frequencies.\"\n\nwordcount = Counter(text.split())\nprint(tabulate(wordcount.most_common(10), headers=['Word', 'Count']))\n\nThis list gives us a nice tabular view of the most frequent words in the text, but how can we get a better sense of the relative frequency of different words (and do it with a fully graphical representation)? For this, we turn to the wordcloud…\n\n#Pseudocode 2: Import the `wordcloud` package and use it to create a wordcloud from the Counter object.\n\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(regexp=r'\\b[^\\W\\d_]+\\b').generate(text)\n\n\n# Display the wordcloud\n\nplt.imshow(wordcloud, interpolation='bilinear');\nplt.axis(\"off\");\n\nWe can adjust the parameters, spec. the max_font_size parameter in the WordCloud class to cram even more words into the same space.\n\n# Display the wordcloud, with lower `max_font_size`\n\nwordcloud = WordCloud(max_font_size=40).generate(text)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n\nLooking at this frequency-based wordcloud, we see exactly the words we expected to see based on the Counter from above: και, τι, τον, γαρ, την, αλλ, ω, δ, αν, των. All important, of course, but carrying no signature of Clouds. It could be any play of Aristophanes, any work on Ancient Greek.\nLuckily we have ways of getting key terms based on a collection of documents, for example tf-idf. A full discussion of tf-idf—or term frequency-inverse document frequency—is out-of-scope for this blog post (though I do have a full tf-idf blog-post explainer coming soon!). For now, let’s take the basic Wikipedia definition as our starting point: we are talking about a “measure of importance of a word to a document in a collection or corpus, adjusted for the fact that some words appear more frequently in general.” The word for “clouds” appears more often Clouds than in the other Aristophanic works. Ditto “frogs” and Frogs. And the ways in which some words appear not merely frequently throughout a collection of documents but within specific documents (and not other!) can be measured using tf-idf.\nLet’s use the approach here, using all of the plays of Aristophanes as ground to Clouds figure.\n\n# Get all of the files of Aristophanes' plays\n\nfiles = [file for file in CR.fileids() if 'aristophanes.' in file]\npprint(files)\nprint()\n\n# Make labels based on the files\nlabels = [file.split('.')[1] for file in files]\npprint(labels)\nprint()\n\ntexts = [preprocess(next(CR.texts(file)), diacriticals=False) for file in files]\nprint(f'There are {len(texts)} texts in this collection.')\n\n\n#Pseudocode 3. Compute the tf-idf scores for each word in all of the plays of Aristophanes.\n\n# In fact we will build a document-term matrix (DTM) with the `CountVectorizer` class from `scikit-learn` first\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nCV = CountVectorizer(input='content', max_df=.90)\ndtm = CV.fit_transform(texts)\nvocab = CV.get_feature_names_out()\n\n# Make dataframe\nimport pandas as pd\ndf_cv = pd.DataFrame(dtm.toarray(), columns=vocab, index=labels)\ndf_cv\n\nHere we have a document-term matrix (DTM) with word counts for each play of Aristophanes. This kind of easy to navigate (if exceptionally large) data structure allows us to see quickly that αβατοισιν and αβατον appears once each in Lysistrata, that αβελτερον once in Ecclesiazusae. There are 11 rows each represent a play and there are 22,312 columns representing our “vocabulary”.\n(It is not a complete vocabulary. Note the parameter max_df=.90 which tells CountVectorizer to ignore words that appear in more than 90% of the documents. This helps remove very high frequency words, not unlike the ones we tended to see in the first batch of wordclouds we generated.)\nBut while this DTM is a convenient data structure for representing—and comparing—documents generally, it is for us here a means to a tf-idf end. Remember from the definition of above that we want to use tf-idf as a way to adjust “for the fact that some words appear more frequently in general” or conversely that some words are more informative within certain documents than within others.”\nThis counts-based DTM can be easily transformed into a tf-idf matrix using the TfidfTransformer class from sklearn…\n\n# Now the tfidf scores\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nTT = TfidfTransformer(use_idf=True).fit(dtm)\ntfidf_dtm = TT.transform(dtm)\ndf_tfidf = pd.DataFrame(tfidf_dtm.toarray(), columns=vocab, index=labels)\ndf_tfidf.iloc[:, :5]\n\n\n# Pseudocode 4. Create a wordcloud based on the key words from *Clouds* (in comparison with the other plays).\n\n# Subset the dataframe to just the *Clouds* row\nclouds = df_tfidf.loc['clouds']\nclouds.sort_values(ascending=False).head(10)\n\nWe can already see some encouraging signs in the tf-idf scores that our keyness-based wordcloud will tell us something about Clouds and not, say, Frogs. We see Socrates. We see the Phrontisterion. A verb of learning tops the list. Perhaps most key of all, we see clouds, or should I say Clouds. (Note how the proper-name character of the Clouds as chorus has been obscured through preprocessing.)\n\n#Pseudocode 4. Create a wordcloud based on the key words from *Clouds* (in comparison with the other plays). (cont.)\n\nCloud = WordCloud(background_color=\"white\", max_words=50).generate_from_frequencies(clouds)\n\n# Display the generated image\nplt.imshow(Cloud, interpolation='bilinear')\nplt.axis(\"off\");\n\nWhat to do next? The clearest next step for this exploration would be to compare the Cloud wordclouds to those for the other Aristophanic works. They are already in memory, ready to be generated. But it is worth asking then—what does it mean to compare one wordcloud to another? We should also check how the wordcloud change when you look at Clouds against a different set of reference texts, say, e.g., the plays of Euripides? all of Greek poetry? etc. This is the kind of read-write-refactor coding style that is at the heart of Exploratory Philology.\n\nReferences\n\n\nCraig, Kalani. 2014. “Teaching Digital Humanities with Analog Tools: Word Clouds and Text Mining.” https://kalanicraig.com/teaching/teaching-digital-humanities-with-analog-tools-word-clouds-and-text-mining/.\n\n\nDunbar, Henry. 1883. A Complete Concordance to the Comedies and Fragments of Aristophanes. Oxford: Claredon Press.\n\n\nHicke, Rebecca M. M., Maanya Goenka, and Eric Alexander. 2022. “Word Clouds in the Wild.” In 2022 IEEE 7th Workshop on Visualization for the Digital Humanities (VIS4DH), 43–48. Oklahoma City, OK, USA: IEEE. https://doi.org/10.1109/VIS4DH57440.2022.00015.\n\n\nMueller, Andreas C. 2020. “Wordcloud.” https://github.com/amueller/wordcloud."
  },
  {
    "objectID": "posts/wills-1996/index.html",
    "href": "posts/wills-1996/index.html",
    "title": "Gemination (Wills 1996)",
    "section": "",
    "text": "In the first part of his 1996 monograph on repetition in Latin poetry, Jeffrey Wills (1996) discusses gemination. i.e. “the repetition of a word in the same form in the same clause with no additional expansion.” In this notebook, we will formalize Wills’ definition of gemination into code using LatinCy.\nLet’s start by setting up a code notebook with Python imports, etc. We will use CLTK Readers with the CLTK-Tesserae texts as our exploratory background for gemination.\n\n# Imports\n\nfrom collections import Counter\nfrom natsort import natsorted\n\nimport spacy\nfrom cltkreaders.lat import LatinTesseraeCorpusReader\n\nfrom latintools import preprocess\n\nfrom tabulate import tabulate\nfrom IPython.core.display import HTML\nfrom IPython.display import display\n\nfrom tqdm import tqdm\n\nWills uses the following line from Virgil’s Eclogues to illustrate gemination (V. Ecl. 2.69):\n\nCorydon, Corydon, quae te dementia cepit!\n\nLet’s begin there.\n\n# Set up corpus\n\nT = LatinTesseraeCorpusReader()\n\n# Get Eclogues file\n\neclogues = [file for file in T.fileids() if 'eclogues' in file][0]\nprint(eclogues)\n\nvergil.eclogues.tess\n\n\nNext we load a LatinCy model to assist with matching Latin wordforms.\n\n# Set up NLP\n\nnlp = spacy.load('la_core_web_lg')\n\n\nSimple gemination\nConsidering Wills’ basic definition from above, we can use the following pseudocode as a starting point…\n\nGet a line of Virgil\n\nCreate a LatinCy Doc for each line\n\nCount the norm token attributes for each line\n\nCheck norm count, i.e. if the count of norm token attributes is greater than 1, then the line has gemination\n\nNote that Wills specifically defines the scope of gemination as a clause (not a line); we will return to this point in a future notebook where we introduce some clause parsing.\nGet a line of Virgil\nFor the Tesserae texts, CLTK Readers has a data structure called doc_rows that, at least for poetry, gives us a dictionary with the format {citation: line, etc.}. Let’s get the docrows for the Eclogues and print a sample line.\n\n# Get all Eclogue rows\n\ndocrows = next(T.doc_rows(eclogues))\n\n\n# Get a row\n\ntest = docrows['&lt;verg. ecl. 2.69&gt;']\nprint(test)\n\nAh, Corydon, Corydon, quae te dementia cepit!\n\n\nWe can already see our gemination—specifically the example Wills uses in his defintion—with the repetition of Corydon.\nCreate a LatinCy Doc\nNext we can create a spaCy Doc for each line. The Doc contains all sorts of annotations useful for philological work. We will use the norm token attribute here to help us match wordforms.\n\n# Create LatinCy Doc for line\n\ndoc = nlp(test)\nprint(type(doc))\n\n&lt;class 'spacy.tokens.doc.Doc'&gt;\n\n\n\n# Print norm examples\nprint(tabulate([[token.i, token.text, token.norm_] for token in doc], headers=['Index','Token', 'Norm']))\n\n  Index  Token     Norm\n-------  --------  --------\n      0  Ah        ah\n      1  ,         ,\n      2  Corydon   corydon\n      3  ,         ,\n      4  Corydon   corydon\n      5  ,         ,\n      6  quae      quae\n      7  te        te\n      8  dementia  dementia\n      9  cepit     cepit\n     10  !         !\n\n\nThinking ahead, if we use lines as is from the Tesserae texts, we have to deal with punctuation. Wills is concerned with the repetition of Corydon, not the repetition of the commas! One way we can deal with this is to preprocess the lines to remove punctuation before creating the Docs. We will discuss the philological implications of preprocessing in a future notebook. For now, we are going to import a script called preprocess that removes punctuation.\n\n# Create LatinCy Doc for preprocessed line and print example\n\ndoc = nlp(preprocess(test, lower=False))\nprint(tabulate([[token.i, token.text, token.norm_] for token in doc], headers=['Index','Token', 'Norm']))\n\n  Index  Token     Norm\n-------  --------  --------\n      0  Ah        ah\n      1  Corydon   corydon\n      2  Corydon   corydon\n      3  quae      quae\n      4  te        te\n      5  dementia  dementia\n      6  cepit     cepit\n\n\nCount the norm token attributes\nWe can now count the norm token attributes for each line using a Counter from the collections module.\n\n# Count `norm` attr in Doc tokens\n\nnorms = [token.norm_ for token in doc]\nnorms_counter = Counter(norms)\nprint(norms_counter)\n\nCounter({'corydon': 2, 'ah': 1, 'quae': 1, 'te': 1, 'dementia': 1, 'cepit': 1})\n\n\nCheck norm count\nWe can now check the norm count for each line. If the count is greater than 1, then the line has gemination.\n\ngeminations = [k for k, v in norms_counter.items() if v &gt; 1]\nprint(f'Number of geminations: {len(geminations)}')\nprint(f'{geminations}')\n\nNumber of geminations: 1\n['corydon']\n\n\nWe knew from Wills that this line would have gemination; of course, not all lines do.\n\n# Try a different line\n\ntest = docrows['&lt;verg. ecl. 2.70&gt;']\ndoc = nlp(preprocess(test))\nnorms = [token.norm_ for token in doc]\nnorms_counter = Counter(norms)\ngeminations = [k for k, v in norms_counter.items() if v &gt; 1]\n\nprint(doc.text)\nprint(f'Number of geminations: {len(geminations)}')\nprint(f'{geminations}')\n\nsemiputata tibi frondosa uitis in ulmo est\nNumber of geminations: 0\n[]\n\n\nHaving worked through our pseudocode, we can now put it all together into a function that we can use to check for gemination in any line of Latin poetry.\n\ndef get_geminations(Doc):\n    norms = [token.norm_ for token in Doc]\n    norms_counter = Counter(norms)\n    geminations = [k for k, v in norms_counter.items() if v &gt; 1]\n    return geminations    \n\nUsing this function, we can loop through the docrows for the Eclogues and check for gemination in each line. In the example below, we break after the first match as we are only checking at this point that the function works as expected.\n\nfor k, v in docrows.items():\n    doc = nlp(preprocess(v, lower=False))\n    geminations = get_geminations(doc)\n    if len(geminations) &gt; 0:\n        print(f'{k}: {geminations}')\n        print(f'{v}')\n        print('\\n')\n        break\n\n&lt;verg. ecl. 1.23&gt;: ['sic']\nsic canibus catulos similis, sic matribus haedos\n\n\n\n\nMore useful of course would be to collect all of the geminations into a data structure like a dictionary…\n\nvirgil_geminations = {}\n\nfor k, v in tqdm(docrows.items()):\n    doc = nlp(preprocess(v))\n    geminations = get_geminations(doc)\n    if geminations:\n        virgil_geminations[k] = (v, geminations)\n\nprint(f'There are {len(virgil_geminations)} geminations in Virgil\\'s *Eclogues*.')\n\n100%|██████████| 828/828 [00:03&lt;00:00, 207.68it/s]\n\n\nThere are 105 geminations in Virgil's *Eclogues*.\n\n\n\nprint('Here are the first five examples from our search:\\n')\nfor k, v in list(virgil_geminations.items())[:5]:\n    print(f'{k}: {v[0]}')\n\nHere are the first five examples from our search:\n\n&lt;verg. ecl. 1.23&gt;: sic canibus catulos similis, sic matribus haedos\n&lt;verg. ecl. 1.33&gt;: nec spes libertatis erat, nec cura peculi:\n&lt;verg. ecl. 1.63&gt;: aut Ararim Parthus bibet, aut Germania Tigrim,\n&lt;verg. ecl. 1.75&gt;: Ite meae, felix quondam pecus, ite capellae.\n&lt;verg. ecl. 2.20&gt;: quam dives pecoris, nivei quam lactis abundans.\n\n\nNote V. Ecl. 1.75 as an example of why we use norm instead of text for matching wordforms. Ite is capitalized here only because it is the first word in the sentence, but should be matched against ite regardless of case. Note the following in Python string matching…\n\nprint('Ite' == 'ite')\nprint('ite' == 'ite')\n\nFalse\nTrue\n\n\nWe can make it easier to see gemination in our texts by formatting matched tokens in HTML. We can use the display module from the IPython package to display the HTML in the notebook.\n\ndef display_gemination(gemination):\n    html = ''\n    line = nlp(gemination[0])\n    terms = gemination[1]\n\n    for token in line:\n        if token.norm_ in terms:\n            token = f'&lt;span style=\"color: green;\"&gt;{token}&lt;/span&gt;'\n        html += f'{token} '\n    return html\n\n\nprint('Here are the first five examples from our search:')\nfor k, v in list(virgil_geminations.items())[:5]:\n    # Note that if you do not remove the angle brackets from the Tesserae citation, it will be ignored as a (bad) HTML tag in the formatting below.\n    citation = k.replace('&lt;', '').replace('&gt;', '') \n    citation = f'&lt;span style=\"color: black; font-weight: bold;\"&gt;{citation}&lt;/span&gt;'\n    text = display_gemination(v)\n    html = '&lt;br&gt;'.join([citation, text])\n    html += '&lt;br&gt;&lt;br&gt;'\n    display(HTML(html))\n\nHere are the first five examples from our search:\n\n\nverg. ecl. 1.23sic canibus catulos similis , sic matribus haedos \n\n\nverg. ecl. 1.33nec spes libertatis erat , nec cura peculi : \n\n\nverg. ecl. 1.63aut Ararim Parthus bibet , aut Germania Tigrim , \n\n\nverg. ecl. 1.75Ite meae , felix quondam pecus , ite capellae . \n\n\nverg. ecl. 2.20quam dives pecoris , nivei quam lactis abundans . \n\n\nMoreover, we can write these matches to a file, formatting the geminations to make them easier to spot, here wrapping repeitions with asterisks.\n\ndef format_gemination(gemination):\n    txt = ''\n    line = nlp(gemination[0])\n    terms = gemination[1]\n\n    for token in line:\n        if token.norm_ in terms:\n            token = f'*{token}*'\n        txt += f'{token} '\n    return txt\n\nwith open('eclogue_geminations.txt', 'w') as f:\n    for k, v in virgil_geminations.items():\n        citation = k.replace('&lt;', '').replace('&gt;', '')\n        citation = f'{citation}'\n        text = format_gemination(v)\n        f.write(f'{citation}\\t{text}\\n')\n\nNote that a line like Ecl. 4.51 appears in the output…\n\nterrasque tractusque maris caelumque profundum !\n\n…as que is considered a token in the LatinCy model.\nAccordingly, we may want to have a way to drop certain tokens from our matching process. We add below an exclude parameter to the get_geminations function to accomplish this.\n\ndef get_geminations(Doc, exclude=[]):\n    norms = [token.norm_ for token in Doc]\n    norms_counter = Counter(norms)\n    geminations = [k for k, v in norms_counter.items() if v &gt; 1 and k not in exclude]\n    return geminations    \n\nexclude =['que']\n\ntest = nlp(preprocess(docrows['&lt;verg. ecl. 4.51&gt;'], lower=False))\n\nprint('Before...')\nprint(get_geminations(test))\nprint()\nprint('After...')\nprint(get_geminations(test, exclude=exclude))\n\nBefore...\n['que']\n\nAfter...\n[]\n\n\nWe write to file again, this time excluding que.\n\nvirgil_geminations = {}\n\nfor k, v in tqdm(docrows.items()):\n    doc = nlp(preprocess(v))\n    geminations = get_geminations(doc, exclude=['que'])\n    if geminations:\n        virgil_geminations[k] = (v, geminations)\n\nwith open('eclogue_geminations.txt', 'w') as f:\n    for k, v in virgil_geminations.items():\n        citation = k.replace('&lt;', '').replace('&gt;', '')\n        citation = f'{citation}'\n        text = format_gemination(v)\n        f.write(f'{citation}\\t{text}\\n')\n\n100%|██████████| 828/828 [00:03&lt;00:00, 213.19it/s]\n\n\nSo far, we have worked only with the Eclogues. We could easily expand this gemination search to other texts in the Tesserae corpus. Here is an example of expanding it to all epic poems in the collection.\n\n# Geminations in all Latin epic\n\n# Note here I get the year from the Tesserae metadata, sort the files chronologically, and then discard the date information\nepic = natsorted([(file, int(T.metadata('date', file))) for file in T.fileids() if T.metadata('genre', file) == 'epic'], key=lambda x: x[1])\nepic = [file for file, _ in epic]\nprint(f'There are {len(epic)} epic poems in the Tesserae collection.')\n\nThere are 120 epic poems in the Tesserae collection.\n\n\n\n# This takes about 7 minutes on my laptop\n\nall_geminations = {}\n\nfor file in tqdm(epic):\n    docrows = next(T.doc_rows(file))\n    for k, v in docrows.items():\n        doc = nlp(preprocess(v))\n        geminations = get_geminations(doc, exclude=['que'])\n        if geminations:\n            all_geminations[k] = (v, geminations)\n\n100%|██████████| 120/120 [06:49&lt;00:00,  3.42s/it]\n\n\n\n# Write to file\nwith open('epic_geminations.tsv', 'w') as f:\n    f.write('citation\\ttext\\n')\n    for k, v in all_geminations.items():\n        citation = k.replace('&lt;', '').replace('&gt;', '')\n        citation = f'{citation}'\n        text = format_gemination(v)\n        f.write(f'{citation}\\t{text}\\n')\n\nThis has been an introduction to formalizing a literary critical/philological argument using LatinCy, an example that barely takes us past the first page of Wills Part I. In subsequent notebooks, we will explore variations on gemination and other types of repetition.\n\n\nReferences\n\n\nWills, Jeffrey. 1996. Repetition in Latin Poetry: Figures of Allusion. Clarendon Press."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploratory Philology Blog",
    "section": "",
    "text": "Aristophanic Wordclouds\n\n\n\n\n\n\n\nexploratory-philology\n\n\nancient-greek\n\n\ntext-analysis\n\n\nvisualization\n\n\n\n\nAn ‘Exploratory Philology’ code notebook showing how to construct wordclouds of key words from Ancient Greek text\n\n\n\n\n\n\nOct 13, 2023\n\n\nPatrick J. Burns\n\n\n\n\n\n\n  \n\n\n\n\nGemination (Wills 1996)\n\n\n\n\n\n\n\nreplicating-classics\n\n\nlatin\n\n\ndata-science\n\n\nallusion\n\n\n\n\nCode notebook providing basic formalization for ‘gemination’ as defined in Wills 1996\n\n\n\n\n\n\nJun 28, 2023\n\n\nPatrick J. Burns\n\n\n\n\n\n\nNo matching items"
  }
]